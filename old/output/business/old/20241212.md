## Overview
I'll explain **step by step**.
~**As you know**, PerL has some audio features, such as Spacial Audio, AAT, and Immersion, and they all are processed in DSP. as a premise.~

This is the screen of QACT which is one of Qualcomm tools. Using this tool, you can see how audio signals pass through these audio features in DSP, like this.
**As you can see**, they basically line up in series and this means multiplication.

## Immersion
Regarding Immersion, ~I remember Steve asked us that what types of filtering does DSP do while Immersion's value are changed via Denon App.~
This is the Frequency Responses graph one of my colleagues has measured the other day.
And turns out that they have differenct characteristics in lower frequency area.

- minus values: it seems that some low shelf filter takes in effect.
- plus values: some Bass Enhancement filter takes in effect.

So ... What type of filters is DSP taking concretely? The answer is...

## Details
Here.
I identified by monitoring firmware logs that
if you set a minus value as immersion value, a processing block named Equalizer reacts.
on the other hand, if plus values, a block named Dynamic Bass reacts.

on top of that, turns out that the Equalizer takes Lower Shelf filter and the Dynamic Bass takes Dynamic Bass as the name suggests.
all of the things considered, I say, this graph totally adds up.

these differences are more than 6 decibel or so
it means that
And each difference is so big that you can also notice the difference in sound when you use each immersion mode value.

And if you want to look into more details, please read this pdf I've attached here.

## words
This is due to the tools spec.
**This is how it is.**
**does that have anything to do with feasibility study?**

## Demonstration
I'll give a demonstration of that.
give me more 3 minutes.

Now, a PerL Pro is connected with my laptop via USB-C. they've been paired with my iPhone as well.
and
this is the tool that shows audio processing streams in the DSP, which is one of Qualcomm software.
and
As you can see, it shows some audio processing blocks and their connections.

And this scenery varies depending on the situation.
Say you call, like this, and you play music, like this.

**The picture I've attached in that confluence page is this. it's hard to see in this state, so I adjust them before attaching it.**

## come clean

and details on these blocks are described in some Qualcomm documents. but I'm not sure what they are because they are more like acoustic things.
So I want to leave these things up to our acoustic team. you know what i mean?

But you can kind of understand what role they have from these words, right?

On top of that, you can see debug logs using another Qualcomm tool.
and you can see application processers give commands to these blocks via each API.
That's the way I identified that.

That's it.

---

i don't know until I look into them more reading some Qualcomm documents or something like that.

I'll `get to it` as soon as I can.

Nothing in particular. thank you.
